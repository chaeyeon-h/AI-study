선형 회귀와 로지스틱 회귀에 대해 살펴보려 한다. 
선형 회귀의 기본 개념에 대해서는 [Ch4-1](https://github.com/Eundding/AI-study/blob/main/Ch4-1%20%EB%AA%A8%EB%8D%B8%20%ED%9B%88%EB%A0%A8%20%7C%20%20%EC%86%90%EC%8B%A4%20%ED%95%A8%EC%88%98%2C%20%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%2C%20BGD%2C%20SGD%2C%20MGD%2C%20Bias-Variance%20Tradeoff.md)에서 이미 다루었기 때문에, 

이번에는 규제가 적용된 선형 회귀를 중심으로 설명할 것이다.

# 규제가 있는 선형 모델
- 선형 회귀 모델에서는 보통 모델의 가중치를 제한함으로써 규제를 가하여 과대적합을 방지한다.
- 가중치가 크다는건 해당 특징이 모델의 예측에 큰 영향을 끼친다는 것이고, 이는 데이터의 잡음까지 포착해서 구불구불한 곡선을 그리게된다.
- 하지만 가중치가 작으면 모델은 보다 부드러운 곡선을 그리게 되고 이는 일반화 성능이 향상됨을 의미한다.
- 과대적합을 줄이는 방법 : 모델을 규제하는 것, 자유도 줄이기
- 다항 회귀 모델 규제 방법 : 다항식의 차수 줄이기

각기 다른 방법으로 가중치를 제한하는 릿지, 라쏘, 엘라스틱넷 회귀를 보자

## 릿지 회귀
- L2규제가 추가된 선형 회귀 버전
- 규제항은 훈련동안에만 비용 함수에 추가되고 훈련이 끝나면 모델의 성능을 규제가 없는 MSE(or RMSE)로 평가한다.
- 입력 특성의 스케일에 민감하기 때문에 수행하기 전에 데이터의 스케일을 맞추는 것이 중요(StandardScaler 사용)
- 하이퍼파라미터 α는 모델이 얼마나 많이 규제할지 조절
- 파라미터 값이 커지는 것을 억제해 과적합(Overfitting) 방지

- **릿지 회귀의 비용 함수**

  - 릿지회귀의 패널티항은 파라미터의 제곱을 더해준 것이. 이것은 미분이 가능해 gradient descent 최적화가 가능하고, 파라미터의 크기가 큰 것을 더 빠른 속도로 줄여준다.
  - α로 규제정도를 조절 (α=0이면 선형회귀)
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/27b445e1-a7ef-481e-bf09-ffc938a3edbf" />

𝐽(𝜃): 전체 비용 함수, 𝑀𝑆𝐸(𝜃): 평균제곱오차

- 오른쪽 다항회귀 그래프에 α의 값이 커질수록 직선에 가까워지는 것을 볼 수 있다.
<img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/47249362-9d64-4525-a546-2c1dd3e97535" />

- 평범한 릿지 모델을 사용해 선형 예측을 만들 수도 있고, PolynomialFeatures(degree=10)를 사용해 데이터를 확장하고 StandardScaler를 사용해 스케일을 조정한 후 적용한 다항 회귀도 가능하다.

 <img width="500" height="600" alt="image" src="https://github.com/user-attachments/assets/cfa7cb14-c858-4b81-bc71-d224ec2b0139" />


**정규방정식**
- 선형 회귀(Linear Regression)에서 경사하강법 없이 최적의 파라미터 θ를 한 번에 구하는 공식. 
비용함수(MSE)를 최소화하도록 θ를 미분하여 얻는다.

## 라쏘 회귀
- L1 규제를 사용하는 선형 회귀 기법
- 릿지 회귀처럼 비용 함수에 규제항을 더하지만 가중치 벡터 L1 노름을 사용
- 라쏘 회귀의 비용 함수
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/2fb08447-2a27-4962-8ced-0378686dac12" />

+) 절댓값 함수는 θ=0에서 미분 불가능하지만, 서브그레이디언트(Subgradient)를 사용해 최적화 가능
- 중요한 특징 중 하나는 덜 중요한 특성의 가중치를 제거하려고 함(즉 가중치가 0이 됨)
- 라쏘를 사용할 때 경사 하강법이 최적점 근처에서 진동하는 것을 막으려면 훈련하는 동안 점진적으로 학습률을 감소시켜야 한다.

<img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/936d8d8d-3476-4a33-9a3c-65bf6d64043d" />
<img width="500" height="600" alt="image" src="https://github.com/user-attachments/assets/a428805f-35b4-4c19-811b-ee2b65860439" />

## 릿지 회귀와 라쏘 회귀 비교
<img width="500" height="600" alt="image" src="https://github.com/user-attachments/assets/c85d8ca4-0582-492e-beb2-59b6e6818ad7" />

- 위 그래프의 두 축은 두 개의 파라미터를 나타낸다. 
- 경사 하강법은 손실 함수의 가장 낮은 지점을 향해 내려가는 알고리즘이다.
- 지금의 경우에는 원점(θ1​=0,θ2​=0)을 향해 내려가는 것을 시각화한 것이다.
- 왼쪽 위 그래프의 등고선은 L1 손실 (∣θ1​∣+∣θ2​∣)을 나타낸다. 축에 뾰족한 모서리를 가지고 있다.
  
  <img width="500" height="601" alt="image" src="https://github.com/user-attachments/assets/f066d803-daa4-4823-8aab-11ad1b9234d2" />
  
- θ1​=2.0, θ2​=0.5인 지점에서 시작한다고 가정한 뒤, 경로를 그려보면 노란색 선과 같이 나타낼 수 있다.
- 따라서 파라미터가 θ1​=2.0, θ2​=0.5와 같은 지점에서 시작하면, 노란 점선처럼 원점까지 직선으로 내려가는 대신, 가장 가파른 방향인 가까운 축을 향해 끌려가게 된다. 즉 축에 가까워지며 선형적으로 줄어든다. (따라서 시작점이 0에 더 가까운 θ2가 먼저 0에 도달)
- 마름모의 가파른 경사를 따라 먼저 θ2​=0인 축으로 이동하고 파라미터 값은 정확히 0이 된다. 그 상태를 유지한 채 θ1​ 축을 따라 원점으로 이동한다.

 <img width="500" height="601" alt="image" src="https://github.com/user-attachments/assets/f91f2397-a5d5-4137-a331-0057fb81d219" />
 
- L1 손실 + MSE 손실을 시각화한 것은 오른쪽 위 그래프이다. 
- θ1 = 0.25, θ2 = - 1로 초기화된 모델 파라미터를 최적화하는 과정을 보자. 
θ2가 빠르게 0으로 빠르게 줄어든 다음 축을 따라 진동하면서 전역 최적점인 빨간 사각형에 도달한다. (L1은 그래디언트는 0에서 정의되지 않기 때문에 진동)
- 라쏘를 사용할 때 경사 하강법이 최적점 근처에서 진동하는 것을 막으려면 훈련하는 동안 점진적으로 학습률을 감소시켜야 한다. 여전히 최적점 근처에서 진동하겠지만 스텝이 점점 작아지므로 수렴하게 될 것이다.
- 아래의 그래프는 L2 규제로 바뀌었고 같은 조건에서 실험이 이루어진다.
- L2​ 페널티(θ1^2​+θ2^2​)의 등고선은 원형이다. 원점을 향해 가장 효율적인 직선 경로로 따라가게 된다.
- 릿지의 경사는 매우 부드럽고 이 때문에 최적점에 도달할 때 진동 없이 안정적으로 수렴할 수 있다.
- L1과 달리 L2는 손실함수의 미분값(그래디언트)이 파라미터 값에 비례한다. 즉, 파라미터(θ)가 0에 가까워질수록, 그래디언트(2θ)도 0에 가까워진다. (L1에서는 그래디언트의 크기가 α에 의해 일정하게 유지됨)
- 하지만 파라미터가 0에 가까워지면 그래디언트가 거의 0이 되기 때문에, 이동 스텝의 크기도 거의 0이 되며 0에 무한히 가까워지지만 0은 되지 않는다.

α가 증가하면: 페널티가 강해지므로, 모델은 파라미터 값을 최소화하는 데 더 집중한다. 이 때문에 최적점(빨간색 사각형)이 원점(0,0)에 가까워진다. (노란색 선을 따라)

α가 감소하면: 페널티가 약해지므로, 최적점은 규제가 없는 일반적인 선형 회귀의 해에 가까워진다.

>**릿지 회귀**는 가중치를 줄여 과적합을 완화하고, 
**라쏘 회귀**는 불필요한 변수를 제거해 희소하고 해석력 높은 모델을 만든다.

## 엘라스틱넷 회귀 _Elastic Net Regression_
- 릿지와 라쏘를 절충한 모델로 릿지 회귀와 라쏘 회귀의 규제항을 더한 것을 규제항으로 사용한다. 
- 혼합 정도를 혼합비율 r을 사용해 조절한다. r = 1에 가까워질수록 라쏘 회귀와 같아진다.

 <img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/5815e4df-0130-44e2-b88e-d451686026eb" />

 - 언제 필요할까?

: 특성 수가 훈련 샘플 수보다 많거나, 특성 몇 개가 강하게 연관되어 있을 경우

```python
from sklearn.linear_model import ElasticNet 
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y) 
elastic_net.predict([[1.5]]) 
array([1.54333232))
```
---
- 릿지 회귀는 L1-norm, 라쏘 회귀는 L2-norm 패널티를 가진다.
- 라쏘는 가중치들이 0이 되지만, 릿지의 가중치들은 0에 가까워질 뿐 0이 되지는 않는다. 
- 특성이 많은데 그중 일부분만 중요하다면 라쏘가, 특성의 중요도가 전체적으로 비슷하다면 릿지를 사용하는 것이 좋다.
- 규제가 있는 것이 대부분의 경우에 일반 선형 회귀보다 좋고, 릿지가 기본이 되지만 일부 특징만 유용하다고 생각된다면 라쏘나 엘라스틱넷을 사용하는 것이 낫다.
- 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 라쏘보다는 엘라스틱넷이 좋다.
---

## 조기 종료 _Early Stopping_
- 검증 오차가 최솟값에 도달하면 훈련을 종료 시키는 것
- 에포크가 진행됨에 따라 점점 훈련세트/검증세트에 대한 오차가 줄어든다.
- 하지만, 일정 수준 이후, 검증 오차가 상승하는데 이는 모델이 훈련 데이터에 과대적합되었다는 것을 의미한다.
  
-> 따라서 적절한 때에 멈춰야할 필요가 있음
<img width="670" height="202" alt="image" src="https://github.com/user-attachments/assets/aa196264-337b-4c1d-a1a2-d2195ffcdb67" />

---
# 로지스틱 회귀
- 이진 분류(Binary Classification)를 위한 알고리즘
+) 원래는 이진 분류용이지만, 소프트맥스 회귀(Softmax Regression, Multinomial Logistic Regression)로 확장하면 다중 클래스 분류도 가능

- 로지스틱은 0과 1 사이의 값을 출력하는 sigmoid 함수(S자 모양(S-shape) 곡선을 그리기 때문에 **시그모이드**라고도 부름)
- 기본 원리 : 입력 특성의 가중치 합을 계산하고 편향을 계산하지만, 선형 회귀처럼 바로 결과를 출력하지 않고, 결괏값의 로지스틱을 출력한다.
- 비용함수
- <img width="620" height="86" alt="image" src="https://github.com/user-attachments/assets/f7ff829e-b68e-4d05-85df-6db4e4992bc6" />

<img width="700" height="300" alt="image" src="https://github.com/user-attachments/assets/e1b76556-6b29-4df5-acf3-3db3ebeca3d1" />

- 50% 이상의 확률값을 가지면 1(양성클래스) , 아니면 0(음성클래스) 의 방식으로 분류를 진행함
- log 손실(Log Loss) 형태이기 때문에 잘못된 확신(확률 0.01인데 정답이 1인 경우)에 큰 벌점 부여
- 볼록(convex) 함수라서 지역 최소값이 전역 최소값
- 따라서 적절한 학습률로 경사 하강법을 사용하면 항상 최적해에 도달 가능

## 훈련과 비용함수
- 목적: 양성 샘플에 대해 높은 확률을 추정하고, 음성 샘플(y=0)에 대해 낮은 확률을 추정하는 모델의 파라미터를 찾는 것
- 전체 훈련 세트에 대한 비용 함수는 모든 훈련 샘플의 비용을 평균한 것

## 결정 경계
- IRIS 데이터 예시
```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split


iris = load_iris(as_frame=True)
x = iris.data[["petal width (cm)”]].values 
y = iris.target_names[iris.target] == 'virginica'
X_train, X_test, y_train, y_test = train_test_split(X, y, andom_state=42)
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

X_new = np.linspace(0, 3, 1000).reshape(-1,1)
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new proba[:, 1] >= 0.5][0, 0] 
decision_boundary 
» 1.6516516516516517 
log_reg.predict([[1.7], [1.5]]) 
» array([True, False])
```

- 모델의 결정 경계를 통해 iris-virginica인지 아닌 지 추정할 수 있다.
- 이 경계는 선형

- **경계 시각화**
  - LogisticRegression의 규제 강도를 조절하는 parameter는 C
<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/484e476d-8998-4131-adac-c046ec9f7ead" />

---
# 소프트맥스 회귀
- 소프트맥스 회귀는 이중 클래스 분류를 다중 클래스 분류로 확장하여 다중 클래스를 직접 지원하는 분류 모델
- 주어진 샘플 x에 대해 각 클래스에 소프트맥스 점수를 계산한 뒤, 소프트맥스 함수를 적용하여 각 클래스의 확률을 추정한다. 가장 확률이 높은 클래스를 최종 예측으로 선택한다.
- 클래스가 𝐾개일 때, 각 클래스에 속할 확률을 모두 동시에 예측
- 클래스 K에 대한 소프트맥스 점수
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/8491af26-41b1-4e5e-a98d-fdaea84f78cd" />

- 점수를 확률로 변환한 후, 확률이 가장 큰 클래스를 예측 값으로 선택
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/693a39af-b5fb-41fe-9bbc-7d8b479fd73c" />
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/059e8436-fb7a-4aae-9f27-4538079b5cfc" />

- 학습을 위한 비용 함수는 아래와 같이 크로스 엔트로피 함수(Cross-Entropy)를 사용한다.
  <img width="500" height="121" alt="image" src="https://github.com/user-attachments/assets/e1004ea8-d469-4d3a-9bac-a650e2c98749" />


## 크로스 엔트로피 _Cross Entropy_
- 모델이 예측한 확률 분포가 실제 정답의 분포와 얼마나 가까운지를 측정한다. 크로스 엔트로피를 최소화하는 것은 곧 모델의 예측을 정답에 가깝게 만드는 과정이다.
- 두 확률 분포 p (실제 정답)와 q (모델 예측)에 대한 크로스 엔트로피(H(p, q))는 다음과 같이 정의된다.
<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/b958751c-58cd-4e32-a939-10c858305503" />


- ex) 붓꽃 3개의 class로 분류
  - 사이킷런의 Logistic Regression은 클래스가 둘 이상일 때 softmax 회귀를 자동으로 적용
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = iris.data[[ "petal length (cm)”, ”petal width (cm)”]].values 
y = iris[target]
X_train, X_test, y_train, y_test = train_test split(X, y, random_state=42) 
softmax_reg = LogisticRegression(C=30, random_state=42)
softmax_reg.fit(X_train, y_train)

# predict() : 특정 class를 반환
# predict_proba() : 클래스 K개에 대해 해당 클래스의 속할 확률을 반환

softmax_reg.predict([[5, 2]]) 
» array([2]) 
softmax_reg.predict_proba([[5, 2]]).round(2)
» array([[0. ,0.04, 0.96]])
```

## 로지스틱 회귀와 소프트맥스 회귀
- 두 회귀는 함수는 본질적으로 동일한 개념을 다룬다. 로지스틱 함수는 소프트맥스 함수가 이진 문제에 적용된 특수한 형태라고 볼 수 있다.

### 1. 소프트맥스 함수
2클래스(class1, class2)일 때 확률은 다음과 같다:

![p1](https://latex.codecogs.com/png.latex?\hat{p}_1=\frac{e^{s_1}}{e^{s_1}+e^{s_2}})  
![p2](https://latex.codecogs.com/png.latex?\hat{p}_2=\frac{e^{s_2}}{e^{s_1}+e^{s_2}})

### 2. 식 변형 (Softmax → Sigmoid)
분자와 분모를 모두 ![es1](https://latex.codecogs.com/png.latex?e^{s_1}) 으로 나누면:

![p1-sigmoid](https://latex.codecogs.com/png.latex?\hat{p}_1=\frac{1}{1+e^{s_2-s_1}})


### 3. 로짓 차이 (Logit Difference)
두 클래스 점수 차이를 ![delta](https://latex.codecogs.com/png.latex?\Delta=s_1-s_2) 라고 하면:

![p1-final](https://latex.codecogs.com/png.latex?\hat{p}_1=\frac{1}{1+e^{-(s_1-s_2)}}=\sigma(s_1-s_2))

즉, **2클래스 softmax는 두 점수 차이를 입력으로 받는 sigmoid와 동일**


### 4. 로지스틱 회귀와의 관계
- 로지스틱 회귀(Logistic Regression)는 **이진 분류**에서 시그모이드 함수를 사용 
- 2클래스 softmax는 sigmoid와 수학적으로 같으므로,  
  **로지스틱 회귀 = 2클래스 softmax 회귀** 

따라서 softmax 회귀는 다중 클래스 확장을 포함할 수 있고, 이 경우 **다항 로지스틱 회귀(Multinomial Logistic Regression)** 라고도 부른다.


**요약**
- Softmax는 여러 클래스 확률을 구하는 함수  
- 2클래스 softmax = sigmoid  
- 로지스틱 회귀 = 2클래스 softmax 회귀  

---
### Study Notes
[이은정](https://velog.io/@dkan9634/HandsOnML-Chap-4.-%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A82)
[안태현](https://armugona.tistory.com/entry/%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-Ch4-%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8-22-%EB%A6%BF%EC%A7%80-%ED%9A%8C%EA%B7%80-%EB%9D%BC%EC%8F%98-%ED%9A%8C%EA%B7%80-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EB%84%B7-%ED%9A%8C%EA%B7%80-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%9A%8C%EA%B7%80) 
[허채연](https://velog.io/@algorithm_cell/%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-ch4-2.-%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8)
